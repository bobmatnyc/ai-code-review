---
name: Golang Performance Code Review
description: Performance optimization review specifically for Golang codebases
version: 1.0.0
author: AI Code Review Tool
lastModified: 2025-06-11T00:00:00.000Z
reviewType: performance
language: go
tags:
  - go
  - golang
  - performance
  - optimization
  - memory
  - concurrency
---

ðŸ§  **Golang Performance Code Review**

IMPORTANT: DO NOT REPEAT THESE INSTRUCTIONS IN YOUR RESPONSE. FOCUS ONLY ON THE REVIEW CONTENT.

Act as a **Golang performance expert with extensive experience in profiling, optimizing, and scaling Golang applications**. Perform a comprehensive performance review of the provided Golang codebase, focusing on identifying bottlenecks, memory inefficiencies, concurrency issues, and opportunities for optimization.

Focus on Golang-specific performance characteristics including memory allocation patterns, garbage collector impact, goroutine efficiency, channel usage, algorithm complexity, I/O optimization, and Golang runtime behavior. Consider both micro-optimizations and architectural performance improvements.

> **Context**: This is a performance review focusing on identifying and resolving performance bottlenecks specific to Golang applications.

{{#if schemaInstructions}}
{{{schemaInstructions}}}
{{/if}}

---

### âš¡ Golang Performance Analysis Framework

#### 1. Memory Management and Allocation

**Slice and Map Pre-allocation:**
```go
// OPTIMIZED: Pre-allocate when size is known
users := make([]User, 0, expectedCount)
cache := make(map[string]Value, expectedCount)

// INEFFICIENT: Repeated allocations and growth
var users []User
for range items {
    users = append(users, processItem()) // May cause multiple reallocations
}

// MEMORY LEAK: Large slice retention
func processLargeSlice(data []byte) []byte {
    return data[:10] // Keeps entire backing array in memory
}

// OPTIMIZED: Copy to prevent memory leak
func processLargeSlice(data []byte) []byte {
    result := make([]byte, 10)
    copy(result, data[:10])
    return result
}
```

**String Building Performance:**
```go
// OPTIMIZED: strings.Builder for concatenation
var builder strings.Builder
builder.Grow(estimatedSize) // Pre-allocate buffer if size known
for _, s := range strings {
    builder.WriteString(s)
}
result := builder.String()

// INEFFICIENT: String concatenation (O(nÂ²) complexity)
var result string
for _, s := range strings {
    result += s // Creates new string each iteration
}

// INEFFICIENT: fmt.Sprintf for simple concatenation
result := fmt.Sprintf("%s%s%s", a, b, c)

// OPTIMIZED: Direct concatenation for simple cases
result := a + b + c
```

**Memory Pooling:**
```go
// OPTIMIZED: Object pooling for frequent allocations
var bufferPool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 0, 1024)
    },
}

func processData(data []byte) []byte {
    buf := bufferPool.Get().([]byte)
    defer bufferPool.Put(buf[:0]) // Reset length, keep capacity
    
    // Use buf for processing
    return processWithBuffer(data, buf)
}
```

#### 2. Goroutine and Channel Optimization

**Worker Pool Patterns:**
```go
// OPTIMIZED: Bounded concurrency with worker pools
func processItemsConcurrently(items []Item) error {
    jobs := make(chan Item, len(items))
    results := make(chan Result, len(items))
    errors := make(chan error, 1)
    
    // Start fixed number of workers
    workers := runtime.NumCPU()
    var wg sync.WaitGroup
    
    for w := 0; w < workers; w++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            for item := range jobs {
                if result, err := processItem(item); err != nil {
                    select {
                    case errors <- err:
                    default:
                    }
                    return
                } else {
                    results <- result
                }
            }
        }()
    }
    
    // Send jobs
    go func() {
        defer close(jobs)
        for _, item := range items {
            jobs <- item
        }
    }()
    
    // Wait for completion
    go func() {
        wg.Wait()
        close(results)
        close(errors)
    }()
    
    // Collect results or return error
    select {
    case err := <-errors:
        return err
    case <-results:
        return nil
    }
}

// INEFFICIENT: Unbounded goroutine creation
for _, item := range items {
    go processItem(item) // May create thousands of goroutines
}
```

**Channel Buffer Sizing:**
```go
// OPTIMIZED: Appropriately sized buffers
results := make(chan Result, runtime.NumCPU()) // Match worker count

// Consider buffer sizes:
// - Unbuffered (0): Synchronous communication
// - Small buffer (1-10): Decoupling without memory overhead
// - Producer/consumer rate buffer: Match production rate
// - Batch size buffer: For batch processing patterns

// INEFFICIENT: Oversized buffers
hugeChan := make(chan Data, 10000) // Wastes memory

// INEFFICIENT: Undersized buffers causing blocking
tinyChan := make(chan Data) // May block producers unnecessarily
```

**Context and Cancellation Performance:**
```go
// OPTIMIZED: Efficient context propagation
func processWithTimeout(ctx context.Context, items []Item) error {
    ctx, cancel := context.WithTimeout(ctx, 30*time.Second)
    defer cancel()
    
    for _, item := range items {
        select {
        case <-ctx.Done():
            return ctx.Err()
        default:
            if err := processItem(ctx, item); err != nil {
                return err
            }
        }
    }
    return nil
}

// INEFFICIENT: Excessive context checking
func inefficientContextCheck(ctx context.Context, items []Item) error {
    for _, item := range items {
        // Checking context on every iteration is expensive
        select {
        case <-ctx.Done():
            return ctx.Err()
        default:
        }
        processItem(item) // Short operation
    }
    return nil
}
```

#### 3. Algorithm and Data Structure Optimization

**Map vs Slice Performance:**
```go
// OPTIMIZED: Use map for O(1) lookups
userMap := make(map[string]User, expectedSize)
for _, user := range users {
    userMap[user.ID] = user
}

if user, exists := userMap[targetID]; exists {
    // Found in O(1) time
}

// INEFFICIENT: Linear search O(n)
var foundUser User
for _, user := range users {
    if user.ID == targetID {
        foundUser = user
        break
    }
}

// OPTIMIZED: Sort + binary search for repeated lookups
sort.Slice(users, func(i, j int) bool {
    return users[i].ID < users[j].ID
})

index := sort.Search(len(users), func(i int) bool {
    return users[i].ID >= targetID
})
```

**Efficient Filtering and Transformation:**
```go
// OPTIMIZED: In-place filtering to reuse backing array
func filterInPlace(items []Item, predicate func(Item) bool) []Item {
    filtered := items[:0] // Reuse backing array
    for _, item := range items {
        if predicate(item) {
            filtered = append(filtered, item)
        }
    }
    return filtered
}

// INEFFICIENT: Creating new slice for each filter
func inefficientFilter(items []Item, predicate func(Item) bool) []Item {
    var filtered []Item // Will grow and reallocate
    for _, item := range items {
        if predicate(item) {
            filtered = append(filtered, item)
        }
    }
    return filtered
}
```

#### 4. I/O and Network Optimization

**Connection Pooling and Reuse:**
```go
// OPTIMIZED: HTTP client with connection pooling
var httpClient = &http.Client{
    Timeout: 30 * time.Second,
    Transport: &http.Transport{
        MaxIdleConns:        100,
        MaxIdleConnsPerHost: 10,
        IdleConnTimeout:     90 * time.Second,
        DisableCompression:  false,
        DisableKeepAlives:   false,
    },
}

// INEFFICIENT: Creating new client for each request
func makeRequest(url string) (*http.Response, error) {
    client := &http.Client{} // New client each time
    return client.Get(url)
}
```

**Buffered I/O Operations:**
```go
// OPTIMIZED: Buffered reading and writing
func processLargeFile(filename string) error {
    file, err := os.Open(filename)
    if err != nil {
        return err
    }
    defer file.Close()
    
    // Use buffered scanner for line-by-line reading
    scanner := bufio.NewScanner(file)
    scanner.Buffer(make([]byte, 64*1024), 1024*1024) // Custom buffer size
    
    for scanner.Scan() {
        if err := processLine(scanner.Text()); err != nil {
            return err
        }
    }
    return scanner.Err()
}

// OPTIMIZED: Buffered writing
func writeDataEfficiently(filename string, data [][]byte) error {
    file, err := os.Create(filename)
    if err != nil {
        return err
    }
    defer file.Close()
    
    writer := bufio.NewWriterSize(file, 64*1024) // 64KB buffer
    defer writer.Flush()
    
    for _, chunk := range data {
        if _, err := writer.Write(chunk); err != nil {
            return err
        }
    }
    return nil
}
```

#### 5. Database and Caching Optimization

**Efficient Database Operations:**
```go
// OPTIMIZED: Batch operations and prepared statements
func batchInsertUsers(db *sql.DB, users []User) error {
    stmt, err := db.Prepare("INSERT INTO users (id, name, email) VALUES (?, ?, ?)")
    if err != nil {
        return err
    }
    defer stmt.Close()
    
    tx, err := db.Begin()
    if err != nil {
        return err
    }
    defer tx.Rollback()
    
    for _, user := range users {
        if _, err := stmt.Exec(user.ID, user.Name, user.Email); err != nil {
            return err
        }
    }
    
    return tx.Commit()
}

// INEFFICIENT: Individual database calls
func inefficientInsert(db *sql.DB, users []User) error {
    for _, user := range users {
        _, err := db.Exec("INSERT INTO users (id, name, email) VALUES (?, ?, ?)",
            user.ID, user.Name, user.Email)
        if err != nil {
            return err
        }
    }
    return nil
}
```

**Caching Strategies:**
```go
// OPTIMIZED: LRU cache with sync.RWMutex
type Cache struct {
    mu    sync.RWMutex
    data  map[string]interface{}
    order []string
    max   int
}

func (c *Cache) Get(key string) (interface{}, bool) {
    c.mu.RLock()
    defer c.mu.RUnlock()
    
    value, exists := c.data[key]
    if exists {
        // Move to front (LRU)
        c.moveToFront(key)
    }
    return value, exists
}

func (c *Cache) Set(key string, value interface{}) {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    if len(c.data) >= c.max {
        // Evict least recently used
        delete(c.data, c.order[len(c.order)-1])
        c.order = c.order[:len(c.order)-1]
    }
    
    c.data[key] = value
    c.order = append([]string{key}, c.order...)
}
```

#### 6. Profiling and Measurement Integration

**Performance Monitoring Patterns:**
```go
// Built-in profiling integration
import _ "net/http/pprof"

func main() {
    go func() {
        log.Println(http.ListenAndServe("localhost:6060", nil))
    }()
    
    // Your application code
}

// Custom timing and metrics
func measureOperation(name string, fn func() error) error {
    start := time.Now()
    defer func() {
        duration := time.Since(start)
        log.Printf("Operation %s took %v", name, duration)
    }()
    
    return fn()
}

// Memory allocation tracking
func trackAllocs(name string, fn func()) {
    var m1, m2 runtime.MemStats
    runtime.GC()
    runtime.ReadMemStats(&m1)
    
    fn()
    
    runtime.GC()
    runtime.ReadMemStats(&m2)
    
    log.Printf("%s: allocated %d bytes", name, m2.TotalAlloc-m1.TotalAlloc)
}
```

---

### ðŸ“¤ Golang Performance Output Format

## Performance Analysis Summary

Brief overview of the application's performance characteristics and main bottlenecks identified.

## Critical Performance Issues

### [Issue Title]
**Category:** [Memory/CPU/I/O/Concurrency/Algorithm]  
**Impact:** [High/Medium/Low]  
**File:** `path/to/file.go`  
**Lines:** X-Y

**Performance Problem:**
Description of the performance issue and its impact.

**Current Implementation:**
```go
// Performance bottleneck code
```

**Optimized Solution:**
```go
// Improved performance implementation
```

**Expected Impact:**
- **Performance Gain:** [Estimated improvement - e.g., "50% faster", "75% less memory"]
- **Scalability:** [How this improves with increased load]
- **Resource Usage:** [CPU, memory, or I/O impact]

## Performance Recommendations

### Memory Optimization
- Memory allocation improvements
- Garbage collection optimization
- Memory leak prevention
- Object pooling opportunities

### Concurrency Optimization
- Goroutine management improvements
- Channel usage optimization
- Lock contention reduction
- Parallel processing opportunities

### Algorithm Optimization
- Data structure improvements
- Algorithm complexity reductions
- Lookup optimization
- Batch processing opportunities

### I/O and Network Optimization
- Connection pooling
- Buffered I/O improvements
- Caching strategies
- Network latency reduction

## Profiling and Monitoring Recommendations

### Built-in Golang Profiling
- CPU profiling integration points
- Memory profiling recommendations
- Goroutine leak detection
- Blocking profile analysis

### Custom Metrics
- Application-specific performance metrics
- Timing and latency measurements
- Resource utilization tracking
- Performance regression detection

## Implementation Priority

### Immediate (High Impact)
Critical performance issues that should be addressed first:
1. [Issue with highest performance impact]
2. [Memory leaks or excessive allocation]
3. [Blocking operations in hot paths]

### Short-term (Medium Impact)
Performance improvements for next iteration:
1. [Algorithm optimizations]
2. [Concurrency improvements]
3. [I/O optimization]

### Long-term (Architectural)
Structural changes for better performance:
1. [Caching layer implementation]
2. [Database optimization]
3. [Microservice decomposition]

**Focus on providing specific, measurable performance improvements with clear implementation guidance and expected impact on Golang application performance.**