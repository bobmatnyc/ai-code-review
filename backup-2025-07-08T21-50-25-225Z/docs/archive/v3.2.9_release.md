# Version 3.2.9 Release Notes

**Release Date**: May 28, 2025  
**Type**: Feature Release with Bug Fixes

## üéØ Overview

Version 3.2.9 introduces support for Anthropic's Claude 4 models (Opus and Sonnet) and fixes critical issues with provider identification and cost calculation. This release ensures accurate provider display and cost estimation for all supported AI models.

## üöÄ New Features

### Claude 4 Model Support

Added support for two new Claude 4 models:

#### Claude 4 Opus (`anthropic:claude-4-opus`)
- **API Identifier**: `claude-opus-4-20250514`
- **Context Window**: 200,000 tokens
- **Pricing**: $15.00 per 1M input tokens / $75.00 per 1M output tokens
- **Description**: Most capable model with advanced reasoning and analysis capabilities

#### Claude 4 Sonnet (`anthropic:claude-4-sonnet`)
- **API Identifier**: `claude-sonnet-4-20250514`
- **Context Window**: 200,000 tokens
- **Pricing**: $3.00 per 1M input tokens / $15.00 per 1M output tokens
- **Description**: Balanced model with enhanced reasoning capabilities

### Usage Examples

```bash
# Use Claude 4 Opus for code review
export AI_CODE_REVIEW_MODEL=anthropic:claude-4-opus
ai-code-review src/

# Use Claude 4 Sonnet (more cost-effective)
export AI_CODE_REVIEW_MODEL=anthropic:claude-4-sonnet
ai-code-review src/

# Estimate costs before running
ai-code-review src/ --model anthropic:claude-4-opus --estimate
```

## üêõ Bug Fixes

### Provider Display Issue
- **Problem**: When using Anthropic models, the provider was incorrectly displayed as "Gemini"
- **Solution**: Updated `ApiClientSelector` to include provider prefix in modelName for all providers
- **Impact**: Provider now correctly displays as "Anthropic" for Claude models

### Cost Calculation Accuracy
- **Problem**: Cost calculations were using incorrect model identifiers
- **Solution**: Updated `AnthropicEstimator` to use API identifiers from modelMaps via `getApiNameFromKey`
- **Impact**: Cost estimates now accurately reflect the model's pricing

### Token Display Mismatch
- **Problem**: Displayed token count didn't match the tokens used for cost calculation
- **Solution**: Enhanced estimation display to show separate input/output token counts
- **Impact**: Users now see exactly what contributes to the cost

### Model Identifier Corrections
- **Problem**: Initial Claude 4 model identifiers didn't match Anthropic's API format
- **Solution**: Corrected identifiers to `claude-opus-4-20250514` and `claude-sonnet-4-20250514`
- **Impact**: Models now work correctly with Anthropic's API

## üí∞ Cost Comparison

### Claude 4 vs Other Models

| Model | Input (per 1M tokens) | Output (per 1M tokens) | Notes |
|-------|----------------------|------------------------|-------|
| Claude 4 Opus | $15.00 | $75.00 | Same as Claude 3 Opus |
| Claude 4 Sonnet | $3.00 | $15.00 | Same as Claude 3.5 Sonnet |
| Claude 3 Opus | $15.00 | $75.00 | Previous flagship |
| Claude 3.5 Sonnet | $3.00 | $15.00 | Popular balanced model |
| GPT-4o | ~$5.00 | ~$15.00 | OpenAI's multimodal |
| Gemini 1.5 Pro | ~$3.50 | ~$10.50 | Google's flagship |

### Example Cost Calculations

For a typical code review of 200 files (~350K tokens):

**Single-pass review:**
- Claude 4 Opus: ~$5.25 (input) + ~$21.00 (output) = **~$26.25**
- Claude 4 Sonnet: ~$1.05 (input) + ~$4.20 (output) = **~$5.25**

**Multi-pass review (3 passes with overhead):**
- Claude 4 Opus: ~$5.54 (input) + ~$22.17 (output) = **~$27.71**
- Claude 4 Sonnet: ~$1.11 (input) + ~$4.43 (output) = **~$5.54**

## üìä Technical Details

### Updated Components

1. **Model Configuration** (`src/clients/utils/modelMaps.ts`)
   - Added Claude 4 model mappings
   - Specified correct API identifiers
   - Set 200K context windows

2. **Cost Estimation** (`src/estimators/anthropicEstimator.ts`)
   - Added Claude 4 pricing
   - Fixed model identifier lookup using `getApiNameFromKey`

3. **Token Analysis** (`src/analysis/tokens/TokenAnalyzer.ts`)
   - Added Claude 4 models to MODEL_CONTEXT_SIZES
   - Set correct context window sizes

4. **Display Logic** (`src/core/reviewOrchestrator.ts`)
   - Enhanced token display to show input/output breakdown
   - Fixed provider extraction in `getProviderDisplayInfo`

5. **API Client** (`src/core/ApiClientSelector.ts`)
   - Updated to include provider prefix in modelName

## üîß Configuration

### Environment Variables

```bash
# Set Claude 4 Opus as default model
export AI_CODE_REVIEW_ANTHROPIC_API_KEY=your-api-key
export AI_CODE_REVIEW_MODEL=anthropic:claude-4-opus

# Or use Claude 4 Sonnet for cost savings
export AI_CODE_REVIEW_MODEL=anthropic:claude-4-sonnet
```

### Command-line Usage

```bash
# Override model for a single run
ai-code-review src/ --model anthropic:claude-4-opus

# Estimate costs before running
ai-code-review src/ --model anthropic:claude-4-sonnet --estimate

# Use multi-pass review without confirmation
ai-code-review src/ --model anthropic:claude-4-opus --no-confirm
```

## üìà Performance Considerations

### Context Window
Both Claude 4 models have a 200K token context window, which typically handles:
- Small projects: Single-pass review
- Medium projects (100-200 files): 2-3 pass review
- Large projects (500+ files): 4+ pass review

### Speed vs Cost Trade-offs
- **Claude 4 Opus**: Best for complex analysis, architectural reviews
- **Claude 4 Sonnet**: Ideal for routine code reviews, quick fixes
- Consider using `--type quick-fixes` with Sonnet for cost-effective reviews

## üîÑ Migration Guide

### From Claude 3 to Claude 4

```bash
# Previous configuration
export AI_CODE_REVIEW_MODEL=anthropic:claude-3-opus

# New configuration (same price, newer model)
export AI_CODE_REVIEW_MODEL=anthropic:claude-4-opus

# Or switch to Sonnet for 5x cost savings
export AI_CODE_REVIEW_MODEL=anthropic:claude-4-sonnet
```

### Cost Optimization Tips

1. **Use Sonnet for routine reviews**: 5x cheaper than Opus
2. **Enable caching**: Use `--use-cache` to avoid re-analyzing unchanged files
3. **Review specific paths**: Target only changed directories
4. **Use appropriate review types**: `--type quick-fixes` uses less output

## üêû Known Issues

1. **High output costs**: Claude 4 Opus output tokens ($75/1M) are among the most expensive
2. **Multi-pass overhead**: Each pass adds ~800 tokens of overhead
3. **No automatic model fallback**: If Claude 4 fails, no automatic fallback to Claude 3

## üîó Links

- **NPM Package**: https://www.npmjs.com/package/@bobmatnyc/ai-code-review
- **GitHub Release**: https://github.com/bobmatnyc/ai-code-review/releases/tag/v3.2.9
- **Documentation**: https://github.com/bobmatnyc/ai-code-review#readme
- **Issue Tracker**: https://github.com/bobmatnyc/ai-code-review/issues

## üìã Checklist for Users

- [ ] Update to version 3.2.9: `npm install -g @bobmatnyc/ai-code-review@latest`
- [ ] Verify installation: `ai-code-review --show-version`
- [ ] Set `AI_CODE_REVIEW_ANTHROPIC_API_KEY` if using Claude models
- [ ] Choose between Opus (powerful) and Sonnet (economical)
- [ ] Test with `--estimate` flag before large reviews
- [ ] Consider using `--no-confirm` for CI/CD pipelines

## üôè Acknowledgments

This release includes contributions from the development session on May 28, 2025, focusing on expanding model support and improving cost transparency for users.

---

*For questions or issues, please visit our [GitHub repository](https://github.com/bobmatnyc/ai-code-review) or open an issue.*